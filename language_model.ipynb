{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.9 64-bit ('text-mining': conda)",
      "metadata": {
        "interpreter": {
          "hash": "95373873a4c48abd0f21d3d87ede5c6fd64f785ed2996579b31b0f133397a09e"
        }
      }
    },
    "colab": {
      "name": "language_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmyMCE0QATPR"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMeSK1i_Adrh",
        "outputId": "b236c86e-538d-41e3-c15c-ff90d54f0e21"
      },
      "source": [
        "device"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm7UOXTwATPa",
        "outputId": "4278dc14-887b-4cbd-e0ef-d7ac5f909cc1"
      },
      "source": [
        "line = \"10/20/19, 14:50 - Robin: Gleich erstmal bei dem Wetter zum Bus laufen ðŸ˜‚\"\n",
        "match = re.findall(r\".+? - (.+?): (.+)\", line)\n",
        "match"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Robin', 'Gleich erstmal bei dem Wetter zum Bus laufen ðŸ˜‚')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaATr19XATPb",
        "outputId": "29a8af6d-09d5-4b21-9a9d-00147d8d2572"
      },
      "source": [
        "v1 = torch.tensor([1,2,3])\n",
        "v2 = torch.tensor([5,45,80])\n",
        "\n",
        "(v1[:, None] + v2[None, :])[0, 2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(81)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apTZuAYGATPc",
        "outputId": "01b4c344-16d3-491c-8e88-863261c0ac95"
      },
      "source": [
        "vectors[0, 1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auEr18yHATPc"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        # input embedding\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx = 0)\n",
        "        # encoder lstms\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first = True, dropout = 0.1, bidirectional = True)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        # obtain embedding of input word\n",
        "        embedding = self.embedding(x)\n",
        "        # get the encoder outputs\n",
        "        output, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
        "\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # init with zeros\n",
        "        return torch.zeros(2 * self.num_layers, batch_size, self.hidden_size, device = device)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9WViUyAATPc"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, max_length, num_layers1, num_layers2):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers1 = num_layers1\n",
        "        self.num_layers2 = num_layers2\n",
        "\n",
        "        # embedding of output words\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx = 0)\n",
        "        # the attention matrix which is applied to the encoder outputs\n",
        "        self.enc_attn_w = nn.Linear(2 * hidden_size, hidden_size)\n",
        "        # the attention matrix which is applied to the current decoder state\n",
        "        self.dec_attn_w = nn.Linear(hidden_size, hidden_size)\n",
        "        # the attention matrix which is applied to the sum of the projected encoder outputs and the current decoder state\n",
        "        self.comb_attn_w = nn.Linear(hidden_size, 1)\n",
        "        self.attn_softmax = nn.Softmax(dim = -1)\n",
        "        # the encoder outputs are double the size of the decoder states because it uses bidirectional layers\n",
        "        # therefore the weighted context vector need to be scaled to half its size before combining it with the decoder state\n",
        "        self.scale_context = nn.Linear(2 * hidden_size, hidden_size)\n",
        "        # the lstm network which is applied to the combination of context vector and decoder state\n",
        "        self.lstm_after_att = nn.LSTM(hidden_size * 3, hidden_size * 3, num_layers2, batch_first = True, dropout = 0.1)\n",
        "        # last layer which projects decoder state to the size of the output vocabulary\n",
        "        self.linear = nn.Linear(hidden_size * 3, output_size)\n",
        "\n",
        "    def forward(self, y, enc_outputs, enc_hidden, hidden1, cell1, hidden2, cell2):\n",
        "        # obtain embedding from lastly predicted symbol\n",
        "        embedding = self.embedding(y)\n",
        "        # obtain unnormalized attention energies by combining the current decoder state with all encoder outputs\n",
        "        attn_energies = torch.tanh(self.dec_attn_w(embedding[:, :, None]) + self.enc_attn_w(enc_outputs[:, None]))\n",
        "        # print(\"Attention sums: \", attn_energies.size())\n",
        "        attn_energies = self.comb_attn_w(attn_energies)\n",
        "        attn_energies = torch.squeeze(attn_energies, dim = -1)\n",
        "        # print(\"Attention energies: \", attn_energies.size())\n",
        "        # obtain weights by normalizing energies\n",
        "        attn_weights = F.softmax(attn_energies, dim = -1)\n",
        "        # apply the weights to the encoder outputs\n",
        "        context = attn_weights[:, :, :, None] * enc_outputs[:, None]\n",
        "        # print(\"Context before sum: \", context.size())\n",
        "        # obtain weighted sum\n",
        "        context = torch.sum(context, dim = -2)\n",
        "        # print(\"Context after sum: \", context.size())\n",
        "        # scale context down to match decoder state size\n",
        "        #context = self.scale_context(context)\n",
        "        # print(\"Context after scaling: \", context.size())\n",
        "        # combine context and decoder state\n",
        "        combined = torch.cat((embedding, context), dim = -1)\n",
        "        # obtain output by applying lstm network and projecting to vocab size\n",
        "        out, (hidden2, cell2) = self.lstm_after_att(combined, (torch.cat((hidden2, enc_hidden), dim = -1), torch.cat((cell2, enc_hidden), dim = -1)))\n",
        "        # print(\"lstm out: \", out.size())\n",
        "        out = self.linear(out)\n",
        "        out = torch.squeeze(out, dim = 1)\n",
        "\n",
        "        return out, hidden1, cell1, hidden2, cell2\n",
        "\n",
        "\n",
        "    def init_hidden1(self, batch_size):\n",
        "        return torch.zeros(self.num_layers1, batch_size, self.hidden_size, device = device)\n",
        "\n",
        "        \n",
        "    def init_hidden2(self, batch_size):\n",
        "        return torch.zeros(self.num_layers2, batch_size, self.hidden_size, device = device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-71784a2316ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roYpgAybATPd"
      },
      "source": [
        "enc = Encoder(10, 64, 4).to(device)\n",
        "dec = Decoder(64, 10, 20, 4, 4).to(device)\n",
        "\n",
        "x = torch.randint(0, 10, (2, 5)).to(device)\n",
        "y = torch.randint(0, 10, (2, 5)).to(device)\n",
        "enc_hidden = enc.init_hidden(2)\n",
        "dec_hidden1 = dec.init_hidden1(2)\n",
        "dec_hidden2 = dec.init_hidden2(2)\n",
        "enc_outputs, enc_hidden, enc_cell = enc(x, enc_hidden, enc_hidden)\n",
        "enc_hidden = enc_hidden.view(4, 2, 2, 64)\n",
        "enc_hidden = torch.cat((enc_hidden[:, 0], enc_hidden[:, 1]), dim = 1).view(4, 2, 128)\n",
        "\n",
        "#print(enc_hidden.size())\n",
        "\n",
        "\n",
        "out, hidden1, cell1, hidden2, cell2 = dec(y, enc_outputs, enc_hidden, dec_hidden1, dec_hidden1, dec_hidden2, dec_hidden2)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hybVo6OATPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19fd4865-6efc-4eb7-b19c-8640f665c187"
      },
      "source": [
        "out.size()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 5, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_ufuTNsATPe"
      },
      "source": [
        "vocab_size = len(dataset.vocab)\n",
        "encoder = Encoder(vocab_size, 128, 3).to(device)\n",
        "decoder = Decoder(128, vocab_size, dataset.max_length, 3, 3).to(device)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAyPcKbVATPf",
        "outputId": "548d8d1c-1d5a-48bc-c40e-7c0d78d7830b"
      },
      "source": [
        "train(encoder, decoder, dataset, epochs = 500, batch_size = 512, hidden_size = 128)"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['nein', 'nein', 'nein', 'ich', 'weis', 'nicht', 'was', 'da', 'falsch', 'ist']\n",
            "['<start>', 'ja', 'und', 'der', '<unk>', '?', 'also', 'die', '4', '<stop>']\n",
            "['ja', 'und', 'der', '<unk>', '?', 'also', 'die', '4', '<stop>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYgrpzLQATPf"
      },
      "source": [
        "def train(encoder, decoder, dataset, epochs = 50, batch_size = 32, hidden_size = 128):\n",
        "\n",
        "    trainloader = DataLoader(dataset, batch_size = batch_size, shuffle = True, drop_last = True)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    encoder_opt = torch.optim.Adam(encoder.parameters())\n",
        "    decoder_opt = torch.optim.Adam(decoder.parameters())\n",
        "\n",
        "    output_length = dataset.max_length\n",
        "\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        running_loss = 0\n",
        "        for i, data in enumerate(trainloader):\n",
        "            input_tensor, output_tensor = data\n",
        "            input_tensor = input_tensor.to(device)\n",
        "            output_tensor = output_tensor.to(device)\n",
        "\n",
        "            encoder_opt.zero_grad()\n",
        "            decoder_opt.zero_grad()\n",
        "            \n",
        "            loss = 0\n",
        "\n",
        "            enc_hidden = encoder.init_hidden(batch_size)\n",
        "            enc_cell = encoder.init_hidden(batch_size)\n",
        "\n",
        "            enc_outputs, enc_hidden, enc_cell = encoder(input_tensor, enc_hidden, enc_cell)\n",
        "            enc_hidden = enc_hidden.view(3, 2, batch_size, hidden_size)\n",
        "            enc_hidden = torch.cat((enc_hidden[:, 0], enc_hidden[:, 1]), dim = 1).view(3, batch_size, hidden_size * 2)\n",
        "\n",
        "            dec_hidden1 = decoder.init_hidden1(batch_size)\n",
        "            dec_cell1 = decoder.init_hidden1(batch_size)\n",
        "            dec_hidden2 = decoder.init_hidden2(batch_size)\n",
        "            dec_cell2 = decoder.init_hidden2(batch_size)\n",
        "\n",
        "            dec_in = output_tensor\n",
        "            target = output_tensor\n",
        "\n",
        "            # print(enc_outputs[0, 0, :20])\n",
        "\n",
        "            dec_out, dec_hidden1, dec_cell1, dec_hidden2, dec_cell2 = decoder(dec_in, \n",
        "                                                                                enc_outputs, \n",
        "                                                                                enc_hidden, \n",
        "                                                                                dec_hidden1, \n",
        "                                                                                dec_cell1, \n",
        "                                                                                dec_hidden2, \n",
        "                                                                                dec_cell2)\n",
        "\n",
        "            loss = criterion(torch.reshape(dec_out[:, :-1, :], (-1, 1054)), torch.reshape(target[:, 1:], (-1,)))\n",
        "\n",
        "            print([dataset.inverse_vocab[t.item()] for t in input_tensor[0, :]])\n",
        "            print([dataset.inverse_vocab[t.item()] for t in target[0, :]])\n",
        "            print([dataset.inverse_vocab[t.item()] for t in torch.argmax(dec_out[0, :-1], dim = -1)])\n",
        "            return\n",
        "            \n",
        "\n",
        "            # for di in range(output_length - 1):\n",
        "            #     dec_in = output_tensor[:, di].view(batch_size, 1)\n",
        "            #     target = output_tensor[:, di]\n",
        "\n",
        "            #     dec_out, dec_hidden1, dec_cell1, dec_hidden2, dec_cell2 = decoder(dec_in, \n",
        "            #                                                                     enc_outputs, \n",
        "            #                                                                     enc_hidden, \n",
        "            #                                                                     dec_hidden1, \n",
        "            #                                                                     dec_cell1, \n",
        "            #                                                                     dec_hidden2, \n",
        "            #                                                                     dec_cell2)\n",
        "\n",
        "            #     loss += criterion(dec_out, output_tensor[:, di + 1])\n",
        "\n",
        "            loss.backward()\n",
        "            encoder_opt.step()\n",
        "            decoder_opt.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if i % 10 == 9:\n",
        "              print(\"Epoch {} - Batch {} - Loss: {}\".format(epoch, i, running_loss / 10))\n",
        "              running_loss = 0\n",
        "\n",
        "    torch.save(encoder.state_dict(), \"encoder.pt\")\n",
        "    torch.save(decoder.state_dict(), \"decoder.pt\")\n",
        "\n"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZDk9snbATPg"
      },
      "source": [
        "def gen_input(message: str, dataset):\n",
        "    tokens = dataset.get_tokens(message)\n",
        "    tokens = [dataset.vocab[token] if token in dataset.vocab else dataset.vocab[\"<unk>\"] for token in tokens]\n",
        "    inp = tokens[:dataset.max_length] + [dataset.vocab[\"<pad>\"]] * max(dataset.max_length - len(tokens), 0)\n",
        "\n",
        "    return torch.tensor(inp)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7L9IQHRATPg"
      },
      "source": [
        "def decode_greedy(inp, encoder, decoder, dataset):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    hidden_size = encoder.hidden_size\n",
        "    output_length = inp.size(0)\n",
        "    batch_size = 1                                                               \n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_hidden = encoder.init_hidden(batch_size)\n",
        "        enc_cell = encoder.init_hidden(batch_size)\n",
        "        enc_in = inp.view(batch_size, -1).to(device)\n",
        "        print(enc_in)\n",
        "\n",
        "        enc_outputs, enc_hidden, enc_cell = encoder(enc_in, enc_hidden, enc_cell)\n",
        "        enc_hidden = enc_hidden.view(3, 2, batch_size, hidden_size)\n",
        "        enc_hidden = torch.cat((enc_hidden[:, 0], enc_hidden[:, 1]), dim = 1).view(3, batch_size, hidden_size * 2)\n",
        "\n",
        "        dec_hidden1 = decoder.init_hidden1(batch_size)\n",
        "        dec_cell1 = decoder.init_hidden1(batch_size)\n",
        "        dec_hidden2 = decoder.init_hidden2(batch_size)\n",
        "        dec_cell2 = decoder.init_hidden2(batch_size)\n",
        "\n",
        "        dec_in = torch.tensor(dataset.vocab[\"<start>\"]).view(1, 1).to(device)\n",
        "        prediction = []\n",
        "\n",
        "        print(enc_outputs[0, 0, :10])\n",
        "\n",
        "        dec_out, dec_hidden1, dec_cell1, dec_hidden2, dec_cell2 = decoder(dec_in, \n",
        "                                                                            enc_outputs, \n",
        "                                                                            enc_hidden, \n",
        "                                                                            dec_hidden1, \n",
        "                                                                            dec_cell1, \n",
        "                                                                            dec_hidden2, \n",
        "                                                                            dec_cell2)\n",
        "        \n",
        "        pred = torch.argmax(dec_out, dim = -1)\n",
        "        prediction.append(dataset.inverse_vocab[pred.item()])\n",
        "\n",
        "        # print(torch.topk(dec_out, 5, 1))\n",
        "\n",
        "        dec_in = pred.view(1, 1)\n",
        "\n",
        "        for di in range(output_length-1):\n",
        "            dec_out, dec_hidden1, dec_cell1, dec_hidden2, dec_cell2 = decoder(dec_in, \n",
        "                                                                            enc_outputs, \n",
        "                                                                            torch.empty(3, batch_size, 0).to(device), \n",
        "                                                                            dec_hidden1, \n",
        "                                                                            dec_cell1, \n",
        "                                                                            dec_hidden2, \n",
        "                                                                            dec_cell2)\n",
        "            #halo\n",
        "            pred = torch.argmax(dec_out, dim = -1)\n",
        "            prediction.append(dataset.inverse_vocab[pred.item()])\n",
        "\n",
        "            # print(torch.topk(dec_out, 5, -1))\n",
        "\n",
        "            dec_in = pred.view(1, 1)\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "encoder = Encoder(1054, 128, 3)\n",
        "encoder.load_state_dict(torch.load(\"baseline_encoder.pt\", map_location=torch.device('cpu')), strict = True)\n",
        "decoder = Decoder(128, 1054, 10, 3, 3)\n",
        "decoder.load_state_dict(torch.load(\"baseline_decoder.pt\", map_location=torch.device('cpu')), strict = True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5STmaYrATPg",
        "outputId": "3da84c7c-1d2e-4fd8-f3f6-54a07c497f53"
      },
      "source": [
        "inp = gen_input(\"wie geht es dir?\", dataset)\n",
        "decode_greedy(inp, encoder, decoder, dataset)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[33, 80, 13, 22,  8,  0,  0,  0,  0,  0]])\ntensor([-0.0158, -0.3362, -0.0093,  0.0240, -0.0389, -0.0253, -0.3156,  0.3089,\n        -0.0141, -0.0667])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['oh',\n",
              " '<unk>',\n",
              " 'ich',\n",
              " 'bin',\n",
              " 'gerade',\n",
              " 'bei',\n",
              " '<unk>',\n",
              " '<stop>',\n",
              " '<pad>',\n",
              " '<pad>']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54g-1SjdATPh"
      },
      "source": [
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, directory = \"chats\", max_length = 10):\n",
        "        self.chats, token_counts = self.get_data(directory)\n",
        "        self.vocab = self.get_vocab(token_counts)\n",
        "        self.inverse_vocab = {val: key for key, val in self.vocab.items()}\n",
        "        self.sequence_pairs = self.get_sequence_pairs(self.chats, max_length)\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def get_sequence_pairs(self, chats, max_length):\n",
        "        sequence_pairs = []\n",
        "        for chat in chats:\n",
        "            # get the number of messages\n",
        "            num_msgs = len(chat[\"Robin\"]) + len(chat[\"Other\"])\n",
        "\n",
        "            start = 0\n",
        "            # iterate start to the first message of \"Other\"\n",
        "            while start in chat[\"Robin\"]:\n",
        "                start += 1\n",
        "            # iterate over all messages\n",
        "            while start < num_msgs:\n",
        "                pair = [[], []]\n",
        "                # add all messages from \"Other\" as first element of pair\n",
        "                while start in chat[\"Other\"]:\n",
        "                    pair[0] += chat[\"Other\"][start]\n",
        "                    start += 1\n",
        "                # add all messages from \"Robin\" as second element of pair\n",
        "                while start in chat[\"Robin\"]:\n",
        "                    pair[1] += chat[\"Robin\"][start]\n",
        "                    start += 1\n",
        "\n",
        "                pair = [[self.vocab[token] if token in self.vocab else self.vocab[\"<unk>\"] for token in sequence] for sequence in pair]\n",
        "                pair[0] = pair[0][:max_length] + [self.vocab[\"<pad>\"]] * max(max_length - len(pair[0]), 0)\n",
        "                pair[1] = [self.vocab[\"<start>\"]] + pair[1][:max_length - 2] + [self.vocab[\"<stop>\"]] + [self.vocab[\"<pad>\"]] * max(max_length - len(pair[1]) - 2, 0)\n",
        "\n",
        "\n",
        "                # append the pair to the sequence pairs\n",
        "                sequence_pairs.append(tuple(pair))\n",
        "\n",
        "        return sequence_pairs\n",
        "        \n",
        "    def get_vocab(self, token_counts):\n",
        "        greater_ten = Counter({token: count for token, count in token_counts.items() if count > 10})\n",
        "        most_common = greater_ten.most_common()\n",
        "        vocab = {token: index + 4 for index, (token, _) in enumerate(most_common)}\n",
        "        vocab[\"<pad>\"] = 0\n",
        "        vocab[\"<unk>\"] = 1\n",
        "        vocab[\"<start>\"] = 2\n",
        "        vocab[\"<stop>\"] = 3\n",
        "        \n",
        "        return vocab\n",
        "    \n",
        "    def get_data(self, directory: str):\n",
        "        chats = []\n",
        "        token_counts = Counter()\n",
        "        for filename in tqdm(Path(directory).glob(\"*.txt\")):\n",
        "            chat, counts = self.get_chat_data(filename)\n",
        "            chats.append(chat)\n",
        "            token_counts += counts\n",
        "        return chats, token_counts\n",
        "\n",
        "\n",
        "    def get_chat_data(self, filename):\n",
        "        f = open(filename, \"r\", encoding = \"utf-8\")\n",
        "        sequences = {}\n",
        "        token_counts = Counter()\n",
        "        index = 0\n",
        "\n",
        "        for line in tqdm(f):\n",
        "            # matches the author and the message\n",
        "            match = re.findall(r\".+? - (.+?): (.+)\", line)\n",
        "            if not match:\n",
        "                continue      \n",
        "            # get author and message\n",
        "            author, msg = match[0]\n",
        "            author = \"Other\" if author != \"Robin\" else \"Robin\"\n",
        "            # skip media files and missed calls\n",
        "            if msg in [\"<Media omitted>\", \"Missed voice call\", \"Missed video call\"]:\n",
        "                continue\n",
        "            # get tokens\n",
        "            tokens = self.get_tokens(msg)\n",
        "\n",
        "            # get token counts\n",
        "            token_counts.update(tokens)\n",
        "\n",
        "            # add entry for author in dictionary\n",
        "            if not author in sequences:\n",
        "                sequences[author] = {}\n",
        "            # add message to dict\n",
        "            sequences[author][index] = tokens\n",
        "            index += 1\n",
        "\n",
        "        return sequences, token_counts\n",
        "\n",
        "    def get_tokens(self, message):\n",
        "        sequence = nltk.word_tokenize(message)\n",
        "        tokens = [token.lower() for token in sequence]\n",
        "        return tokens\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequence_pairs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        seq1, seq2 = self.sequence_pairs[index]\n",
        "\n",
        "        return (torch.tensor(seq1), torch.tensor(seq2))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kTfZ89bApOW",
        "outputId": "b11a3f0e-b9c4-41c5-9222-c0c5902b2b4e"
      },
      "source": [
        "!unzip chats.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  chats.zip\n",
            "   creating: chats/\n",
            "  inflating: chats/sophie.txt        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siApFi6fAt_Y",
        "outputId": "ab49a03a-4aac-45de-e17e-0a82a64e333d"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhMgTNYEATPh",
        "outputId": "69d7c404-00a4-47e0-8616-b0ce2eaf633a"
      },
      "source": [
        "dataset = ChatDataset(max_length = 10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1382it [00:00, 13205.02it/s]\u001b[A\n",
            "2771it [00:00, 13528.15it/s]\u001b[A\n",
            "4162it [00:00, 13641.50it/s]\u001b[A\n",
            "5583it [00:00, 13813.47it/s]\u001b[A\n",
            "6965it [00:00, 13621.86it/s]\u001b[A\n",
            "8328it [00:00, 13538.09it/s]\u001b[A\n",
            "9736it [00:00, 13671.21it/s]\u001b[A\n",
            "11214it [00:00, 13979.00it/s]\u001b[A\n",
            "12620it [00:00, 13962.94it/s]\u001b[A\n",
            "14017it [00:01, 13904.08it/s]\u001b[A\n",
            "15455it [00:01, 14007.55it/s]\u001b[A\n",
            "16856it [00:01, 13926.89it/s]\u001b[A\n",
            "18249it [00:01, 13925.75it/s]\u001b[A\n",
            "19656it [00:01, 13603.90it/s]\u001b[A\n",
            "21019it [00:01, 13575.08it/s]\u001b[A\n",
            "22378it [00:01, 13342.84it/s]\u001b[A\n",
            "24534it [00:01, 13669.83it/s]\n",
            "1it [00:01,  1.81s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpm6GpjWATPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f199122-b9d1-466c-d3be-16b026e8fc6e"
      },
      "source": [
        "dataset[10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([  9,   5, 178,   4,  75, 372, 157, 161,  75,  11,  77, 167, 381,  37,\n",
              "          25, 472, 694,   0,   0,   0]),\n",
              " tensor([ 2,  1, 21,  1, 33,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}