{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FmyMCE0QATPR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import nltk\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from xml.dom import minidom\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import operator\n",
    "import gensim\n",
    "import itertools\n",
    "\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kTfZ89bApOW"
   },
   "outputs": [],
   "source": [
    "# for use in google colab\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "54g-1SjdATPh"
   },
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    \"\"\"Dataset which contains tuples of questions and answers.\n",
    "    \n",
    "    This dataset extracts sequences from txt files. The txt files need to store one pair\n",
    "    of question and answer per line in the following form:\n",
    "    \n",
    "    \"['question_1','question_2',...]\\t['answer_1','answer_2',...]\\n\"\n",
    "    \n",
    "    Each line contains a list of questions and answers, because a question can consist of \n",
    "    multiple messages, e.g. in a WhatsApp chat one user might send multiple consecutive messages\n",
    "    without an answer in between.\n",
    "    \n",
    "    The dataset returns tuples of questions and answers, where questions and answers are given\n",
    "    as lists of unique word indices. Multiple consecutive questions and answers are linked using\n",
    "    the <new> token to symbolize the start of a new message.\n",
    "    \n",
    "    \n",
    "    Functions:\n",
    "        __get_data: loads data from a directory with txt files.\n",
    "        __get_sequences: loads tuples of questions and answers from specific txt file.\n",
    "        __get_vocab: creates vocabulary and pretrains word2vec model for the loaded sequences.\n",
    "        get_embeddigns: returns the pretrained embeddigns of the word2vec model.\n",
    "        \n",
    "    Attributes:\n",
    "        max_length: the maximum length of a sequence.\n",
    "        embedding_size: the size of the pretrained embeddings.\n",
    "        vocab: maps tokens to unique indices.\n",
    "        inverse_vocab: maps indices to tokens.\n",
    "        word2vec: trained gensim word2vec model.\n",
    "    \"\"\"\n",
    "    def __init__(self, directory, max_length = 10, embedding_size = 128):\n",
    "        \"\"\"Initialize Dataset\n",
    "        \n",
    "        Get all sequences (questions and answers) from a directory of text files, \n",
    "        create vocabulary and pretrain word2vec embeddings.\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        print(\"READING TXT FILES\")\n",
    "        data = self.get_data(directory)\n",
    "        print(\"OBTAINING EMBEDDINGS AND VOCABULARY\")\n",
    "        self.vocab, self.word2vec = self.get_vocab(data)\n",
    "        self.inverse_vocab = {val: key for key, val in self.vocab.items()}\n",
    "        print(\"DATA LOADED\")\n",
    "        \n",
    "    def __get_data(self, directory):\n",
    "        \"\"\"Get tuples of questions and answers from directory of txt files.\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        for filename in tqdm(list(Path(directory).glob(\"*.txt\"))):\n",
    "            sequences = self.get_sequences(filename)\n",
    "            data += sequences\n",
    "            \n",
    "        return data\n",
    "        \n",
    "    def __get_sequences(self, filename):\n",
    "        \"\"\"Get tuples of questions and answers from specific txt file.\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        \n",
    "        with open(filename, \"r\", encoding = \"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    # each line consists of [question_1, ...]\\[answer_1, ...]\\n\n",
    "                    question_group, answer_group = re.findall(\"(.+?)\\t(.+?)\\n\", line)[0]\n",
    "                except:\n",
    "                    # skip lines with false formatting\n",
    "                    continue\n",
    "                    \n",
    "                # parse the strings to Python lists, \n",
    "                #e.g. question_group = [\"hey\", \"how are you?\"] and answer_group = [\"good\", \"what about you?\"]\n",
    "                question_group, answer_group = eval(question_group), eval(answer_group)\n",
    "                # tokenize each message,\n",
    "                # e.g. question_group = [[\"hey\"], [\"how\", \"are\", \"you\", \"?\"]]\n",
    "                question_group, answer_group = [nltk.word_tokenize(question) for question in question_group], [nltk.word_tokenize(answer) for answer in answer_group]\n",
    "\n",
    "                sequences.append((question_group, answer_group))\n",
    "\n",
    "        return sequences\n",
    "        \n",
    "    def __get_vocab(self, data):\n",
    "        # unzip to obtain question groups and answer groups\n",
    "        question_groups, answer_groups = list(zip(*data))\n",
    "        \n",
    "        # gather all sequences from all groups into a single list of sentences\n",
    "        questions = [question for question_group in question_groups for question in question_group]\n",
    "        answers = [answer for answer_group in answer_groups for answer in answer_group]\n",
    "        sentences = questions + answers\n",
    "        \n",
    "        #train word2vec model on sentences and remove infrequent words \n",
    "        word2vec = gensim.models.Word2Vec(sentences, iter = 100, window = 8, size = self.embedding_size, min_count = 5)\n",
    "        \n",
    "        # obtain vocabulary including special tokens\n",
    "        vocab = {token: index + 5 for index, token in enumerate(word2vec.wv.index2word)}\n",
    "        vocab[\"<pad>\"] = 0\n",
    "        vocab[\"<unk>\"] = 1\n",
    "        vocab[\"<start>\"] = 2\n",
    "        vocab[\"<stop>\"] = 3\n",
    "        vocab[\"<new>\"] = 4\n",
    "        \n",
    "        return vocab, word2vec\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        \"\"\" Return the pretrained embeddings together with 0-initialized embeddings for the 5 special tokens\n",
    "        \"\"\"\n",
    "        embeddings = torch.cat((torch.zeros((5, self.embedding_size)), torch.FloatTensor(self.word2vec.wv.vectors)))\n",
    "        return embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return size of dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Obtain tuple at given index\n",
    "        \"\"\"\n",
    "        question_group, answer_group = self.data[index]\n",
    "        # link together sequences of one group with the <new> token\n",
    "        # e.g. question_group = [[\"hey\"], [\"how\", \"are\", \"you\", \"?\"]] -> question = [\"hey\", \"<new>\", \"how\", \"are\", \"you\", \"?\"]\n",
    "        question, answer = [[token for seq in seq_group for token in seq + [\"<new>\"]] for seq_group in [question_group, answer_group]]\n",
    "        # replace every token with its unique index or with the <unk> index if it is not in the vocabulary\n",
    "        question, answer = [[self.vocab[token] if token in self.vocab else self.vocab[\"<unk>\"] for token in seq] for seq in [question, answer]]\n",
    "\n",
    "        # either cut off long sequences or pad short sequences so that every sequence has length max_length\n",
    "        question = question[:self.max_length] + [self.vocab[\"<pad>\"]] * max(self.max_length - len(question), 0)\n",
    "        # additionally, add sos and eos tokens to start and end of the answer\n",
    "        answer = [self.vocab[\"<start>\"]] + answer[:self.max_length - 2] + [self.vocab[\"<stop>\"]] + [self.vocab[\"<pad>\"]] * max(self.max_length - len(answer) - 2, 0)  \n",
    "    \n",
    "        return (torch.tensor(question), torch.tensor(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wi4TIkAsRsqu"
   },
   "outputs": [],
   "source": [
    "# example for initializing a dataset from a directory \"chats\" which contains txt files\n",
    "wa_dataset = ChatDataset(\"chats\")\n",
    "# print question and answer of the 9th dataset item\n",
    "print(list(wa_dataset.inverse_vocab[token.item()] for token in wa_dataset[8][0]))\n",
    "print(list(wa_dataset.inverse_vocab[token.item()] for token in wa_dataset[8][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "auEr18yHATPc"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder for a Seq2Seq network.\n",
    "    \n",
    "    Takes a sequence of word indices as input and obtains the embeddings. The embeddings\n",
    "    are then passed through a bi-LSTM network to produce a sequence of encoder vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, pretrained_emb = None):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # initialize embeddings, if given\n",
    "        if pretrained_emb is None:\n",
    "            self.embedding = nn.Embedding(input_size, hidden_size, padding_idx = 0)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_emb)\n",
    "            \n",
    "        # bi-LSTM network with num_layers layers and dropout\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first = True, dropout = 0.1, bidirectional = True)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        \"\"\"Forward pass of the encoder.\n",
    "        \n",
    "        Args:\n",
    "            x: sequence of word indices of shape (batch, seq_len)\n",
    "            hidden: last hidden state of the bi-LSTM of shape (2 * num_enc_layers, batch, hidden_size)\n",
    "            cell: last cell state of the bi-LSTM of shape (2 * num_enc_layers, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        # obtain embedding of input index\n",
    "        embedding = self.embedding(x)\n",
    "        # get the encoder outputs\n",
    "        output, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "\n",
    "        return output, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # initialize hidden state with zeros\n",
    "        return torch.zeros(2 * self.num_layers, batch_size, self.hidden_size, device = device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "q9WViUyAATPc"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder for a Seq2Seq network.\n",
    "    \n",
    "    Takes as input the lastly predicted output index and obtains the embedding. The embedding\n",
    "    then attends over the sequence of encoder vectors and produces a context vector. Finally, \n",
    "    the embedding and the context vector are concatenated and passed to an LSTM network. Additionally,\n",
    "    the last encoder state is added to the previous decoder state in every time step.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, output_size, num_layers, pretrained_emb = None):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # initialize embeddings, if given\n",
    "        if pretrained_emb is None:\n",
    "            self.embedding = nn.Embedding(output_size, hidden_size, padding_idx = 0)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_emb)\n",
    "            \n",
    "        # the weights for the Bahdanau attention mechanism\n",
    "        # i.e. energy(s_t', h_t) = comb_attn_w * (enc_attn_w * h_t + dec_attn_w * s_t'),\n",
    "        # where s_t' is the decoder state at time t' and h_t is the encoder state at time t\n",
    "        self.enc_attn_w = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.dec_attn_w = nn.Linear(hidden_size, hidden_size)\n",
    "        self.comb_attn_w = nn.Linear(hidden_size, 1)\n",
    "        # softmax for normalizing attention energies\n",
    "        self.attn_softmax = nn.Softmax(dim = -1)\n",
    "        \n",
    "        # linear layer for scaling down the last encoder state in order to add it to the previous decoder state\n",
    "        # (bc the encoder is bidirectional, its states are double the size of decoder states)\n",
    "        self.scale_enc_hidden = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        \n",
    "        # LSTM network with num_layers layers\n",
    "        self.lstm = nn.LSTM(hidden_size * 3, hidden_size, num_layers, batch_first = True, dropout = 0.1)\n",
    "        \n",
    "        # last layer which projects decoder state to the size of the output vocabulary\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, y, enc_outputs, enc_hidden, hidden, cell):\n",
    "        \"\"\"Forward pass through the decoder.\n",
    "        \n",
    "        Args:\n",
    "            y: lastly predicted output with shape (batch, 1)\n",
    "            enc_outputs: sequence of encoder vectors of shape (batch, seq_len, hidden_size * 2)\n",
    "            enc_hidden: the last encoder output of shape (batch, 1, hidden_size * 2)\n",
    "            hidden: the last hidden state of the lstm network of shape (num_dec_layers, batch, hidden_size)\n",
    "            cell: the last cell state of the lstm network of shape (num_dec_layers, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        # obtain embedding from lastly predicted symbol\n",
    "        embedding = self.embedding(y)\n",
    "        \n",
    "        # obtain unnormalized attention energies by using Bahdanau attention\n",
    "        # attn_energies is of shape (batch, 1, seq_len)\n",
    "        # and contains energies for current decoder state and all encoder states\n",
    "        attn_energies = torch.tanh(self.dec_attn_w(embedding[:, :, None]) + self.enc_attn_w(enc_outputs[:, None]))\n",
    "        attn_energies = self.comb_attn_w(attn_energies)\n",
    "        attn_energies = torch.squeeze(attn_energies, dim = -1)\n",
    "        \n",
    "        # obtain attention scores by normalizing energies\n",
    "        attn_weights = F.softmax(attn_energies, dim = -1)\n",
    "        # weigth encoder outputs with attention scores\n",
    "        context = attn_weights[:, :, :, None] * enc_outputs[:, None]\n",
    "        # obtain weighted sum\n",
    "        context = torch.sum(context, dim = -2)\n",
    "        \n",
    "        # concatenate context and embedding\n",
    "        combined = torch.cat((embedding, context), dim = -1)\n",
    "        # scale last encoder state by a factor of 2\n",
    "        enc_hidden = self.scale_enc_hidden(enc_hidden)\n",
    "        \n",
    "        # apply lstm network\n",
    "        out, (hidden, cell) = self.lstm(combined, (hidden + enc_hidden, cell))\n",
    "        # project to output vocabulary\n",
    "        out = self.linear(out)\n",
    "        # remove seq_len dimension\n",
    "        out = torch.squeeze(out, dim = 1)\n",
    "\n",
    "        return out, hidden, cell\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # initialize hidden state with zeros\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYgrpzLQATPf"
   },
   "outputs": [],
   "source": [
    "def train(encoder, decoder, dataset, epochs = 500, batch_size = 512):\n",
    "    \"\"\"Train Seq2Seq network on a given dataset\n",
    "    \"\"\"\n",
    "    # define trainloader\n",
    "    trainloader = DataLoader(dataset, batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "\n",
    "    # use cross entropy as loss and Adam as optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    encoder_opt = torch.optim.Adam(encoder.parameters())\n",
    "    decoder_opt = torch.optim.Adam(decoder.parameters())\n",
    "\n",
    "    vocab_size = len(dataset.vocab)\n",
    "    hidden_size = encoder.hidden_size\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get data and move it to the device\n",
    "            input_tensor, output_tensor = data\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            output_tensor = output_tensor.to(device)\n",
    "\n",
    "            encoder_opt.zero_grad()\n",
    "            decoder_opt.zero_grad()\n",
    "            \n",
    "            loss = 0\n",
    "\n",
    "            # init encoder hidden state and cell\n",
    "            enc_hidden = encoder.init_hidden(batch_size)\n",
    "            enc_cell = encoder.init_hidden(batch_size)\n",
    "            # obtain encoder outputs\n",
    "            enc_outputs, enc_hidden, enc_cell = encoder(input_tensor, enc_hidden, enc_cell)\n",
    "            # concatenate the last hidden states from both directions\n",
    "            enc_hidden = enc_hidden.view(encoder.num_layers, 2, batch_size, hidden_size)\n",
    "            enc_hidden = torch.cat((enc_hidden[-1, 0], enc_hidden[-1, 1]), dim = 1).view(1, batch_size, hidden_size * 2)\n",
    "\n",
    "            # init decoder hidden and cell state\n",
    "            dec_hidden = decoder.init_hidden(batch_size)\n",
    "            dec_cell = decoder.init_hidden(batch_size)\n",
    "\n",
    "            # pass the indices from the target sentence into the decoder, one at a time\n",
    "            for i in range(output_tensor.size(1) - 1):\n",
    "                dec_in = output_tensor[:, i].view(-1, 1)\n",
    "                # use teacher forcing\n",
    "                target = output_tensor[:, i + 1].view(-1, 1)\n",
    "\n",
    "                # produce next decoder output\n",
    "                dec_out, dec_hidden, dec_cell = decoder(dec_in, enc_outputs, enc_hidden, dec_hidden, dec_cell)\n",
    "\n",
    "                # add to loss\n",
    "                loss += criterion(torch.reshape(dec_out, (-1, vocab_size)), torch.reshape(target, (-1,)))\n",
    "\n",
    "            # do backpropagation and update weights\n",
    "            loss.backward()\n",
    "            encoder_opt.step()\n",
    "            decoder_opt.step()\n",
    "\n",
    "            # add to running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # print current mean loss after every epoch\n",
    "        print(\"Epoch {} - Loss: {}\".format(epoch, running_loss / ((len(dataset) // batch_size) * dataset.max_length)))\n",
    "\n",
    "    # save the models\n",
    "    torch.save(encoder.state_dict(), \"encoder.pt\")\n",
    "    torch.save(decoder.state_dict(), \"decoder.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZDk9snbATPg"
   },
   "outputs": [],
   "source": [
    "def gen_input(message: str, dataset):\n",
    "    \"\"\"Generate encoder input from message.\n",
    "    \"\"\"\n",
    "    tokens = dataset.get_tokens(message)\n",
    "    tokens = [dataset.vocab[token] if token in dataset.vocab else dataset.vocab[\"<unk>\"] for token in tokens]\n",
    "    inp = tokens[:dataset.max_length - 1] + [dataset.vocab[\"<new>\"]] + [dataset.vocab[\"<pad>\"]] * max(dataset.max_length - len(tokens) - 1, 0)\n",
    "\n",
    "    return torch.tensor(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmBGZ1RRLMiO"
   },
   "outputs": [],
   "source": [
    "def decode_beam(inp, encoder, decoder, dataset, beam_width):\n",
    "    \"\"\"Return the beam_width top predictions of the decoder given an input.\n",
    "    \"\"\"\n",
    "    # go into eval mode (disable dropout)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    hidden_size = encoder.hidden_size\n",
    "    batch_size = 1\n",
    "\n",
    "    top_picks = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # init encoder inputs\n",
    "        enc_hidden = encoder.init_hidden(batch_size)\n",
    "        enc_cell = encoder.init_hidden(batch_size)\n",
    "        enc_in = inp.view(batch_size, -1).to(device)\n",
    "\n",
    "        # obtain encoder outputs\n",
    "        enc_outputs, enc_hidden, enc_cell = encoder(enc_in, enc_hidden, enc_cell)\n",
    "\n",
    "        #prepare hidden encoder state for decoder\n",
    "        enc_hidden = enc_hidden.view(3, 2, batch_size, hidden_size)\n",
    "        enc_hidden = torch.cat((enc_hidden[:, 0], enc_hidden[:, 1]), dim = 1).view(3, batch_size, hidden_size * 2)\n",
    "\n",
    "        # init decoder inputs\n",
    "        dec_hidden = decoder.init_hidden(batch_size)\n",
    "        dec_cell = decoder.init_hidden(batch_size)\n",
    "        dec_in = torch.tensor(dataset.vocab[\"<start>\"]).view(1, 1).to(device)\n",
    "\n",
    "        # obtain first decoder outputs\n",
    "        dec_out, dec_hidden, dec_cell = decoder(dec_in, enc_outputs, enc_hidden, dec_hidden, dec_cell)\n",
    "\n",
    "        # get first top predictions\n",
    "        top_k = torch.topk(F.softmax(dec_out, 1), beam_width, 1)\n",
    "\n",
    "        # save parameters of the first top picks\n",
    "        top_picks = [{\n",
    "            \"seq\": [token],\n",
    "            \"prob\": np.log(prob.item()),\n",
    "            \"hid\": dec_hidden,\n",
    "            \"cell\": dec_cell,\n",
    "        } for prob, token in zip(top_k[0][0], top_k[1][0])]\n",
    "\n",
    "        # do 10 decoding steps\n",
    "        for i in range(10):\n",
    "            hypotheses = []\n",
    "\n",
    "            # go through every current top pick\n",
    "            for pick in top_picks:\n",
    "                # the lastly predicted symbol is the next input\n",
    "                dec_in = pick[\"seq\"][-1].view(1, 1)\n",
    "\n",
    "                # get the next outputs\n",
    "                dec_out, dec_hidden, dec_cell = decoder(dec_in, enc_outputs, enc_hidden, pick[\"hid\"], pick[\"cell\"])\n",
    "\n",
    "                # get next top picks of the current hypothesis\n",
    "                top_k = torch.topk(F.softmax(dec_out, 1), beam_width, 1)\n",
    "\n",
    "                # store parameters of the top picks\n",
    "                picks = [{\n",
    "                    \"seq\": pick[\"seq\"] + [token],\n",
    "                    \"prob\": np.log(prob.item()) + pick[\"prob\"],\n",
    "                    \"hid\": dec_hidden,\n",
    "                    \"cell\": dec_cell,\n",
    "                } for prob, token in zip(top_k[0][0], top_k[1][0])]\n",
    "\n",
    "                # add to current hypothesis\n",
    "                hypotheses += picks\n",
    "\n",
    "            # sort after probability\n",
    "            hypotheses = sorted(hypotheses, key = operator.itemgetter(\"prob\"), reverse = True)\n",
    "\n",
    "            # get top k hyptheses\n",
    "            top_picks = hypotheses[:beam_width]\n",
    "\n",
    "    for pick in top_picks:\n",
    "        print(np.exp(pick[\"prob\"].item()), [dataset.inverse_vocab[token.item()] for token in pick[\"seq\"]])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "k5STmaYrATPg",
    "outputId": "c1dd4620-4d28-4484-f9f7-1ccc6039ed4b"
   },
   "outputs": [],
   "source": [
    "# example of how predictions can be obtained from an input sentence\n",
    "inp = gen_input(\"hallo\", wa_dataset)\n",
    "decode_beam(inp, encoder, decoder, wa_dataset, 10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "language_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "002488e9a9bc4667834e16c22852abe9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "055ce169a4e94a1580389e0f6b1c016c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2fff8afcf40b4010805f8369ba47fec2",
      "placeholder": "​",
      "style": "IPY_MODEL_59bee32ff11344d6b798596c13d5f76f",
      "value": " 200/200 [05:41&lt;00:00,  1.71s/it]"
     }
    },
    "29d68f733bc5414c8c37c7ecabac484d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c8eb6e521b7494e93e8d8d9d63558ac": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fff8afcf40b4010805f8369ba47fec2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c3935b67b9e4c29bc054e325bda9418": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d549315600dc4d3ebda1eeb3fc7520fd",
       "IPY_MODEL_d7c2f2f6f7a54c1db343c579d917e609"
      ],
      "layout": "IPY_MODEL_2c8eb6e521b7494e93e8d8d9d63558ac"
     }
    },
    "59bee32ff11344d6b798596c13d5f76f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "82c24b5cb7264b0abf3998bae4c0af92": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b9008bc9f3894096b702a206746a9d1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_002488e9a9bc4667834e16c22852abe9",
      "max": 200,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d33807e33e4f4a9e8e4f9299937b77f2",
      "value": 200
     }
    },
    "d33807e33e4f4a9e8e4f9299937b77f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d549315600dc4d3ebda1eeb3fc7520fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29d68f733bc5414c8c37c7ecabac484d",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_82c24b5cb7264b0abf3998bae4c0af92",
      "value": 100
     }
    },
    "d7c2f2f6f7a54c1db343c579d917e609": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ecae299c398e4ebb8780aa95103d900b",
      "placeholder": "​",
      "style": "IPY_MODEL_f3e0ef1641974e829d3a2ddf84e02785",
      "value": " 100/100 [04:04&lt;00:00,  2.45s/it]"
     }
    },
    "e49bf37a1f3c4f69b9a88f547a76d58b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b9008bc9f3894096b702a206746a9d1c",
       "IPY_MODEL_055ce169a4e94a1580389e0f6b1c016c"
      ],
      "layout": "IPY_MODEL_f5c3692d553b49ffbb67e053858da820"
     }
    },
    "ecae299c398e4ebb8780aa95103d900b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3e0ef1641974e829d3a2ddf84e02785": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5c3692d553b49ffbb67e053858da820": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
